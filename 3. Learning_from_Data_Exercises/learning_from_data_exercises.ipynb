{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"text-align: center\">  Learning From Data Exercises </div> \n",
    "<img src=\"https://blogs.elespectador.com/wp-content/uploads/2017/09/logo-Universidad-Nacional.png\" \n",
    "     style=\"float: right; margin-right: 10px;\" \n",
    "     width=\"120\"\n",
    "     />\n",
    "\n",
    "<div style=\"text-align: left\"> \n",
    "Edison David Serrano Cárdenas. <br>\n",
    "Student of Mathematics and Computer Sciences. <br>\n",
    "Universidad Nacional de Colombia - Sede Bogotá <br>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The objective of this notebook is show the solution of some of the exercises of the book [Learning From Data - by Yaser S. Abu-Mostafa, Malik Magdon-Ismail, Hsuan-Tien Lin](https://www.amazon.com/Learning-Data-Yaser-S-Abu-Mostafa/dp/1600490069).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: The Learning Problem \n",
    "\n",
    "## Exercises"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2:\n",
    "\n",
    "<img src=\"https://scholarlyoa.com/wp-content/uploads/2020/01/spam-messages-1024x561.jpg\" \n",
    "     style=\"float: right; margin-right: 10px;\" \n",
    "     width=\"400\"\n",
    "     />\n",
    "     \n",
    "Suppose that we use a perceptron to detect spam messages. Let's say that each email message is represented by the frequency of occurrence of keywords, and the output is +1 if the message is considered spam.\n",
    "\n",
    "(a) Can you think of some keywords that will end up with a large positive weight in the perceptron?\n",
    "\n",
    "(b) How about keywords that will get a negative weight?\n",
    "\n",
    "(c) What parameter in the perceptron directly affects how many borderline messages end up being classified as spam?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "(a) Some keywords with a large positive weight: free, cheap, earn, !, unlimited, urgent.\n",
    "\n",
    "(b) Keywords with a negative weight: person name, hi, the, person university name.\n",
    "\n",
    "(c) The parameter in perceptron directly affects how many borderline messages end up being classified as spam. This is because is the threshold used to classify the emails into spam and non-spam categories."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.3:\n",
    "\n",
    "The weight update rule in $(1.3)$ has the nice interpretation that it moves in the direction of classifying $\\mathbf{x}(t)$ correctly.\n",
    "\n",
    "(a) Show that $y(t) \\mathbf{w}^{\\mathrm{T}}(t) \\mathbf{x}(t)<0$. [Hint: $\\mathbf{x}(t)$ is misclassified by $\\mathbf{w}(t)$.]\n",
    "\n",
    "(b) Show that $y(t) \\mathbf{w}^{\\mathrm{T}}(t+1) \\mathbf{x}(t)>y(t) \\mathbf{w}^{\\mathrm{T}}(t) \\mathbf{x}(t)$. [Hint: Use (1.3).]\n",
    "\n",
    "(c) As far as classifying $\\mathbf{x}(t)$ is concerned, argue that the move from $\\mathbf{w}(t)$ to $\\mathbf{w}(t+1)$ is a move 'in the right direction'.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "\n",
    "(a) If $x(t)$ is misclassified by $w(t)$, then $w^T(t)x(t)$ has different signs of $y(t)$, thus $y(t)w^T(t)x(t) \\gt 0$.\n",
    "\n",
    "(b) In fact, \n",
    "\\begin{align*}\n",
    "y(t)w^T(t+1)x(t) &= y(t) \\left(w(t)+y(t)x(t)\\right)^Tx(t) \\\\\n",
    "&= y(t)\\left(w^T(t) + y(t)x^T(t)\\right)x(t) \\\\\n",
    "&= y(t)w^T(t)x(t) + y(t)y(t)x^T(t)x(t)\\\\\n",
    "&\\gt y(t)w^T(t)x(t) \\;\\;\\;\\text{because the last term is } \\ge \\text{ than } 0\\\\\n",
    "\\end{align*}\n",
    "\n",
    "(c) From previous problem, we see that $y(t)w^T(t)x(t)$ is increasing with each update. \n",
    "\n",
    "If $y(t)$ is positive, but $w^T(t)x(t)$ is negative, we move $w^T(t)x(t)$ toward positive by increasing it. If however $y(t)$ is negative, but $w^T(t)x(t)$ is positive, $y(t)w^T(t)x(t)$ increases means $w^T(t)x(t)$ is decreasing, i.e. moving toward negative region. \n",
    "\n",
    "So the move from $w(t)$ to $w(t+1)$ is a move \"in the right direction\" as far as classifying $x(t)$ is concerned."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.11\n",
    "\n",
    "We are given a data set $\\mathcal{D}$ of 25 training examples from an unknown target function $f: \\mathcal{X} \\rightarrow \\mathcal{Y}$, where $\\mathcal{X}=\\mathbb{R}$ and $\\mathcal{Y}=\\{-1,+1\\}$. To learn $f$, we use a simple hypothesis set $\\mathcal{H}=\\left\\{h_1, h_2\\right\\}$ where $h_1$ is the constant +1 function and $h_2$ is the constant -1 .\n",
    "\n",
    "We consider two learning algorithms, S (smart) and C (crazy). S chooses the hypothesis that agrees the most with $\\mathcal{D}$ and $C$ chooses the other hypothesis deliberately. Let us see how these algorithms perform out of sample from the deterministic and probabilistic points of view. Assume in the probabilistic view that there is a probability distribution on $\\mathcal{X}$, and let $\\mathbb{P}[f(\\mathbf{x})=+1]=p$.\n",
    "\n",
    "(a) Can S produce a hypothesis that is guaranteed to perform better than random on any point outside $\\mathcal{D}$ ?\n",
    "\n",
    "(b) Assume for the rest of the exercise that all the examples in $\\mathcal{D}$ have $y_n=+1$. Is it possible that the hypothesis that $\\mathrm{C}$ produces turns out to be better than the hypothesis that $\\mathrm{S}$ produces?\n",
    "\n",
    "(c) If $p=0.9$, what is the probability that $S$ will produce a better hypothesis than $\\mathrm{C}$ ?\n",
    "\n",
    "(d) Is there any value of $p$ for which it is more likely than not that $C$ will produce a better hypothesis than $S$ ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "(a) $S$ can not produce a hypothesis that is guaranteed to perform better than random on any point outside $\\mathcal{D}$. \n",
    "If $f$ has 25 $+1$ on $\\mathcal{D}$ but $-1$ on all other points in $\\mathcal{X}$, $S$ will choose the hypothesis $h_1$, which will not match $f$ outside of $\\mathcal{D}$ at all. On the other hand, a random function will have $+1$ and $-1$ 50/50, and it matches $f$ half of time, which is better than the function produced by $S$.\n",
    "\n",
    "(b) It is possible that $C$ produces a better hypothesis than $S$ produces. See the example above.\n",
    "\n",
    "(c) If every point in $\\mathcal{D}$ has 1, then $S$ will choose $h_1$ and $C$ will choose $h_2$. So outside of $\\mathcal{D}$, $h_1$ will have 90% chance to match with $f$, while $h_2$ will have only 10% chance. $S$ will always produce a better hypothesis than $C$. \n",
    "\n",
    "(d) From previous problem, we can see that when $p \\lt 0.5$, $C$ will produce a better hypothesis than $S$. Since $C$ always produce $h_2$, which will match $f$ better than $h_1$ if $p \\lt 0.5$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.12\n",
    "\n",
    "A friend comes to you with a learning problem. She says the target function $f$ is completely unknown, but she has 4,000 data points. She is willing to pay you to solve her problem and produce for her a $g$ which approximates $f$. What is the best that you can promise her among the following:\n",
    "\n",
    "(a) After learning you will provide her with a $g$ that you will guarantee approximates $f$ well out of sample.\n",
    "\n",
    "(b) After learning you will provide her with a $g$, and with high probability the $g$ which you produce will approximate $f$ well out of sample.\n",
    "\n",
    "(c) One of two things will happen.\n",
    "(i) You will produce a hypothesis $g$;\n",
    "(ii) You will declare that you failed.\n",
    "If you do return a hypothesis $g$, then with high probability the $g$ which you produce will approximate $f$ well out of sample."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Solution:**\n",
    "\n",
    "I think the best I can promise is (c). \n",
    "* The unknown target $f$ can be very complex that we can't learn at all.\n",
    "* If we can learn and produce a hypothesis $g$, since there are many data points (4000), the probability that $g$ matches $f$ is high according to Hoeffding inequality, and the error on $g$ might be small since we have a large data set."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.13\n",
    "\n",
    "Consider the bin model for a hypothesis $h$ that makes an error with probability $\\mu$ in approximating a deterministic target function $f$ (both $h$ and $f$ are binary functions). If we use the same $h$ to approximate a noisy version of $f$ given by\n",
    "$$\n",
    "P(y \\mid \\mathbf{x})= \\begin{cases}\\lambda & y=f(\\mathbf{x}) \\\\ 1-\\lambda & y \\neq f(\\mathbf{x})\\end{cases}\n",
    "$$\n",
    "(a) What is the probability of error that $h$ makes in approximating $y$ ?\n",
    "\n",
    "(b) At what value of $\\lambda$ will the performance of $h$ be independent of $\\mu$ ? [Hint: The noisy target will look completely random.]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Solution:**\n",
    "\n",
    "(a) The probability of error that $h$ makes in approximating $y$ is\n",
    "\n",
    "\\begin{align*}\n",
    "P(h \\ne y) &= P(h \\ne y| y = f(x))P(y=f(x)) + P(h\\ne y| y \\ne f(x))P(y\\ne f(x))\\\\\n",
    "&= \\mu \\lambda + (1-\\mu)(1-\\lambda)\\\\\n",
    "&= \\mu (2 \\lambda - 1) + (1-\\lambda)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "(b) It can be seen from previous problem, that when $\\lambda = 0.5$, $P(h\\ne y) = 1-\\lambda = 0.5$, is independent of $\\mu$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "[1] Learning From Data (Machine Learning course - recorded at a live broadcast from Caltech) - source: https://work.caltech.edu/telecourse<br>\n",
    "[2] Abu-Mostafa, Y. S., Magdon-Ismail, M., & Lin, H. T. (2012). Learning from data (Vol. 4, p. 4). New York: AMLBook. <br>\n",
    "[3] Learning-From-Data-A-Short-Course. Neil Z. Source: https://github.com/niuers/Learning-From-Data-A-Short-Course/blob/master/Solutions%20to%20Chapter%201%20The%20Learning%20Problem.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
